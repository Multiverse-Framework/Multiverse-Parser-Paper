<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies.">
  <meta name="keywords" content="Universal Scene Description, Semantic Scene Understanding, Knowledge Graphs">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Multiverse Parser</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/index.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8/dist/styles.css"/>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/pdf/2310.16737">
            Multiverse Knowledge
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://ai.uni-bremen.de/team/hoang_giang_nguyen">Giang Nguyen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://ai.uni-bremen.de/team/mihai_pomarlan">Mihai Pomarlan</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://ai.uni-bremen.de/team/sascha_jongebloed">Sascha Jongebloed</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://ai.uni-bremen.de/team/nils_leusmann">Nils Leusmann</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.acin.tuwien.ac.at/en/staff/mnv">Minh Nhat Vu</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://ai.uni-bremen.de/team/michael_beetz">Michael Beetz</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Institute for Artificial Intelligence, University of Bremen, Germany,</span>
            <span class="author-block"><sup>2</sup>Appliable Linguistics, University of Bremen, Germany</span>
            <span class="author-block"><sup>3</sup>Automation & Control Institute, TU Wien, Austria</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2507.11770"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2507.11770"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Multiverse-Framework/Multiverse-Parser"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/IROS2025.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The generation of multiple scene descriptions from a USD scene graph (dataset: <a href="https://github.com/LightwheelAI/Lightwheel_Kitchen">Lightwheel Kitchen</a>).
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p> 
            In robotics, the effective integration of environ-
            mental data into actionable knowledge remains a significant
            challenge due to the variety and incompatibility of data formats
            commonly used in scene descriptions, such as MJCF, URDF,
            and SDF. 
          </p>
          <p>
            This paper presents a novel approach that addresses
            these challenges by developing a unified scene graph model
            that standardizes these varied formats into the Universal Scene
            Description (USD) format. This standardization facilitates the
            integration of these scene graphs with robot ontologies through
            semantic reporting, enabling the translation of complex environmental 
            data into actionable knowledge essential for cognitive
            robotic control.
          </p>
          <p>
            We evaluated our approach by converting
            procedural 3D environments into USD format, which is then
            annotated semantically and translated into a knowledge graph
            to effectively answer competency questions, demonstrating its
            utility for real-time robotic decision-making. Additionally, we
            developed a web-based visualization tool to support the semantic 
            mapping process, providing users with an intuitive interface
            to manage the 3D environment.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <img src="./static/images/VisualAbstract.png" alt="Visual Abstract" style="max-width:80%; height:auto;">
    <p class="subtitle">Figure 1: The generation of a Knowledge Graph from various ontologies derived from a USD Scene Graph.</p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Our methodology starts with gathering environmental data
            from various sources, such as ProcTHOR, which is then
            combined into standardized scene graphs. 
            These graphs are then semantically
            enriched by mapping them with robot ontologies facilitated 
            by semantic reporting. Semantic reporting automatically 
            identifies relevant concepts from preloaded ontologies
            based on attributes like names or geometries in the scene
            graphs. Following the concepts filtered through semantic
            reporting, semantic labeling can be efficiently executed.
            Once labeled, these semantically enriched scene graphs are
            converted into knowledge graphs, equipping robotic agents
            with the necessary understanding to interact efficiently with
            their environment. This enriched semantic data allows robots
            to perform tasks more precisely and dynamically adapt to
            new situations. Our approach is designed for scalability
            and adaptability, accommodating continuous updates and
            refinements for complex robotic interactions.
          </p>
          <div style="display: flex; flex-direction: column; align-items: center;">
            <div style="display: flex; justify-content: center; align-items: center; gap: 2rem;">
              <img src="./static/images/USD_Illustrate.png" alt="USD Illustration" style="max-width:50%; height:auto;">
              <img src="./static/images/Architecture.png" alt="Architecture" style="max-width:50%; height:auto;">
            </div>
            <p class="subtitle has-text-centered" style="margin-top: 1rem;">Figure 2: Diagram of the methodology</p>
          </div>
          <div style="height: 2rem;"></div>
          <p>
            The pipeline architecture described in this paper is built
            upon existing workflows. It starts with an ambiguous command 
            such as ”Please prepare a breakfast for me,” which
            a natural language processor converts into executable tasks.
            These tasks are executed by the plan execution, which loads
            environmental data, including the robot and its surroundings,
            to create a scene graph. This scene graph is subsequently
            transformed into a knowledge graph and fed into a reasoner
            to enable essential competency questions for task execution
            in real time. 
          </p>
          <p>
            The following sections will explain how to translate a
            scene graph into a knowledge graph. This includes parsing
            common scene description formats into USD and tagging
            semantic labels derived by semantic reporting using a tool
            developed in this paper.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Universal Scene Description Parser</h2>
        <div class="content has-text-justified">
          <p>
            Integrating diverse scene descriptions into various sim-
            ulation environments and their connection to knowledge
            graphs necessitates a translation mechanism. We streamline
            this process by employing USD as an intermediary format,
            minimizing the coupling of different parsers and enhancing
            consistency.
          </p>
          <div class="container is-max-desktop has-text-centered">
            <img src="./static/images/MultiverseParser.png" alt="Multiverse Parser" style="max-width:80%; height:auto;">
            <p class="subtitle">Figure 3: Storing various scene descriptions into USD and converting them into others in different simulation and visualization platforms.</p>
          </div>
          <div style="height: 2rem;"></div>
          <p>
            In practice, simulators should selectively utilize scene graph features, 
            focusing on essential elements and omitting unnecessary ones. 
            For example, RViz uses URDF for scene visualization without needing 
            joint dynamics or physics properties, while MuJoCo, focused on physical interactions, 
            excludes resource-intensive visual and non-collidable meshes. 
            
            Simplifying data acquisition and computations by ignoring non-essential meshes 
            and materials is practical when the primary focus is physical behavior rather 
            than model accuracy. However, challenges arise when scene descriptions lack or 
            have poorly designed dynamic properties, complicating accurate simulation outcomes.

            To address these challenges, refining the scene graphs is an essential final step in the translation process. 
            This involves improving dynamic properties through estimations based on mesh data, 
            using B. Mirtich's method for fast and accurate polyhedral mass properties computation. 
            The algorithm computes mass, the center of mass, and inertia for uniform-density 
            polyhedra by converting mass integrals into volume integrals, reducing numerical inaccuracies. 
            Complex inertial chains are consolidated into a single root body inertia following the parallel 
            axes theorem.
          </p>
        </div>
      </div>
    </div>
  </div>
  <div style="height: 2rem;"></div>
  <div class="container is-max-desktop">
    <img-comparison-slider>
      <img slot="first" src="./static/images/Apartment_IsaacSim.png" />
      <img slot="second" src="./static/images/Apartment_MuJoCo.png" />
    </img-comparison-slider>
    <p class="has-text-centered">Apartment Lab from USD to MJCF</p>
    <div class="columns is-multiline is-centered">
      <div class="column is-half">
        <img-comparison-slider>
          <img slot="first" src="./static/images/Neo_IsaacSim.png" />
          <img slot="second" src="./static/images/Neo_MuJoCo.png" />
        </img-comparison-slider>
        <p class="has-text-centered">Neo from USD to MJCF</p>
      </div>
      <div class="column is-half">
        <img-comparison-slider>
          <img slot="first" src="./static/images/GR1_T1_IsaacSim.png" />
          <img slot="second" src="./static/images/GR1_T1_MuJoCo.png" />
        </img-comparison-slider>
        <p class="has-text-centered">GR1_T1 from USD to MJCF</p>
      </div>
      <div class="column is-half">
        <img-comparison-slider>
          <img slot="first" src="./static/images/phoenix_IsaacSim.png" />
          <img slot="second" src="./static/images/phoenix_MuJoCo.png" />
        </img-comparison-slider>
        <p class="has-text-centered">Phoenix from USD to MJCF</p>
      </div>
      <div class="column is-half">
        <img-comparison-slider>
          <img slot="first" src="./static/images/aws_IsaacSim.png" />
          <img slot="second" src="./static/images/aws_MuJoCo.png" />
        </img-comparison-slider>
        <p class="has-text-centered">JetBot from USD to MJCF</p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Labeling scene graphs with semantic reports</h2>
        <div class="content has-text-justified">
          <p>
            Building on the knowledge graph construction method, 
            we generate a USD layer that contains a
            collection of ontological concepts defined in the preloaded
            ontologies for semantic tagging. Additionally, we update
            the schema SemanticTagAPI with a new property for storing semantic reports. 
            These enhancements facilitate semantic tagging with reduced man-
            ual effort, as explained below. The main USD scene graph,
            which is translated from the previous section, and the USD
            layer for semantic tagging are visualized using a web-based
            tool we developed (<a href="https://github.com/Multiverse-Framework/Multiverse-View">Multiverse-View</a>).
          </p>
        </div>
        <div class="container is-max-desktop has-text-centered">
          <img src="./static/images/MultiverseView.png" alt="Multiverse View" style="max-width:80%; height:auto;">
          <p class="subtitle">Figure 4: A snapshot of the web-based visualization tool displaying a USD scene for semantic tagging.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visual Effects</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      
      <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2>

        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
        
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>

      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{giang2025multiverseparser,
  author    = {Giang Nguyen, Mihai Pomarlan, Sascha Jongebloed, Nils Leusmann, Minh Nhat Vu and Michael Beetz},
  title     = {Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies},
  journal   = {IROS},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
